{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is all about gathering data from the internet that follows a very structured format. Code is very good at handling predictable cases since you can break things down to if/else statements. Now, how do we get this data, and how should we store it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, I'm going to assume you're fluent in Python and have already set up the project environment. The vast majority of this tutorial (if not all of it) will only regard the \"data\" folder inside of the root folder of the project. Feel free to open this folder in VS Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the web scraper, you can type \"python index.py\" or \"python3 index.py\" if you're on Mac/Linux while inside the data folder. That means that the code we start with starts in the index.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has 4 main parts to it:\n",
    "- Imports\n",
    "- Constants\n",
    "- Functions\n",
    "- \\_\\_main\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one you're probably familiar with. We start by importing external libraries, argparse, os, and unittest (all of which come built into Python).\n",
    "Then we import functions/classes from our local \"data\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.scrape import scrape_data\n",
    "from util.cache import HTMLCache\n",
    "from util.utils import time_function\n",
    "from util.logger import enable_logs\n",
    "from model.database import Database\n",
    "from model.indexer import Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first import, from model.scrape import scrape_data, is from the \"scrape.py\" file inside of the model folder. The second, from util.cache import HTMLCache, is from the \"cache.py\" file from the util folder. Why do we separate all of these Python files into different folders? Organization. That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get to constants, which are pretty self-explanatory, and then we get to the main function. This is the meat of our scraper code. It's a function that takes in args and returns None. If you've never dealt with args, no worries, we'll get there. Underneath the \"main\" function is where our code *actually* starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--reset_cache\", action=\"store_true\")\n",
    "    parser.add_argument(\"--only_tests\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_tests\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_scraping\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_indexing\", action=\"store_true\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    parser.add_argument(\"--data_path\", default=DEFAULT_DATA_PATH)\n",
    "    parser.add_argument(\"--database_filename\", default=DEFAULT_DATABASE_FILENAME)\n",
    "    parser.add_argument(\"--index_filename\", default=DEFAULT_INDEX_FILENAME)\n",
    "    parser.add_argument(\"--vocabulary_filename\", default=DEFAULT_VOCABULARY_FILENAME)\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't remember what the if statement is checking for, it just checks if the current file is the one being ran rather than being imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the program by parsing a bunch of arguments, which let us configure how we run the application without us having to modify our code. If we want to reset our cache before we run the code, instead of doing it manually, all we have to do now is run the program like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python index.py --reset_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we parse the arguments, we call the main function. I'll break it down piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: argparse.Namespace) -> None:\n",
    "    # run the testing sweet unless it should be skipped\n",
    "    if not args.skip_tests:\n",
    "        # look through the ./tests folder for unit tests\n",
    "        test_suite = unittest.TestLoader().discover('./tests')\n",
    "        test_result = unittest.TextTestRunner(verbosity=0).run(test_suite)\n",
    "        \n",
    "        if len(test_result.errors) > 0 or len(test_result.failures):\n",
    "            print(\"Test suite failed, quitting program\")\n",
    "            exit()\n",
    "\n",
    "        # if we only wanted to run tests, end the program here\n",
    "        if args.only_tests:\n",
    "            exit()\n",
    "    else:\n",
    "        print(\"Skipping tests, good luck!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the testing suite. If the tests fail, that means the rest of the code will probably fail, so we just quite the program early. The tests are located inside of the tests folder, and their organized into separate files based on what they're supposed to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want extra logs, enable them\n",
    "if args.verbose:\n",
    "    enable_logs()\n",
    "\n",
    "# create the data folder if it doesn't exist\n",
    "if not os.path.exists(args.data_path):\n",
    "    os.mkdir(args.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then enable/disable extra logs and create the database folder if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the cache, reset if necessary\n",
    "cache = HTMLCache(reset=args.reset_cache)\n",
    "\n",
    "# create the database\n",
    "database = Database(args.data_path, args.database_filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create 2 very important objects. The first is a cache. What's this cache for? We're going to be downloading a lot of data from the internet, and one thing you'll learn is that downloading stuff is very slow, and we want to reduce how many times we do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the cache does is every time it downloads something from the internet, say the HTML code for the website \"www.google.com\", it saves that code for a file on your local disk, and maps the url \"www.google.com\" to where it is in that file. This way, the next time you want the HTML code for \"www.google.com\", instead of downloading it from the internet again, it can just read it from your local disk, which is a lot faster than downloading it again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you want to download the HTML code for \"www.google.com\" multiple times? Well let's say you ran your scraper, and then made some changes to it and want to run it again. If you run it again after making those changes, you would probably be looking through the same urls that you were looking through earlier, right? This means that if you re-run your code, it won't have to re-download all of those old websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question you might have is, well if the page on the internet changes, like the logo for \"www.google.com\" changing, will the HTML code in our cache change to reflect that? No, since I didn't program it to. This might be a problem if the website that we're downloading data from changes a lot, but the UCI General Catalogue doesn't change very often, maybe once a quarter/year. We can pass in the --reset_cache flag when we run the code to delete the cache files and re-download the newest version of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the database object? It's not too complicated luckily, it's basically a Python dictionary that knows how to save/load itself from a file. That's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.skip_scraping:\n",
    "    # begin scraping \n",
    "    time_function(scrape_data, cache, database)\n",
    "else:\n",
    "    # if we wanted to load a previous database instead\n",
    "    # of scraping all over again, load here\n",
    "    database.load_course_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start scraping stuff from the internet. If you look at the time_function function from the utility class, it just logs to the console how long it took to run that function, in this case, scrape_data. Let's look at scrape_data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(cache: HTMLCache, database: Database) -> None:\n",
    "    courses = scrape_courses(cache)\n",
    "\n",
    "    log(\"Updating course database\")\n",
    "    # store the courses into the database\n",
    "    for course in courses:\n",
    "        database.update_course_data(course.id, course)\n",
    "    log(\"Saving course database\")\n",
    "    database.save_course_data()\n",
    "    log(\"Saved database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is pretty simple. It scrapes a list of courses from the internet, which will be a list of CourseData objects. It then updates the database with these courses and saves the database to the disk as a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how scrape_courses works now. It has the same structure as a lot of the code we've looked at:\n",
    "- Imports\n",
    "- Constants\n",
    "- Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports are probably pretty self-explanatory, but we have this external one called BeautifulSoup. This is where the real magic of our scraper comes from. BeautifulSoup turns HTML code (which is usually a string from a file), into easily navigable objects called soups, or BeautifulSoups. You can read more about it on its documentation page: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start at the scrape_courses function in the scrape_courses.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes all of the courses from the COURSE_URL page and returns a list of CourseData objects\n",
    "def scrape_courses(cache: HTMLCache) -> list:\n",
    "    log(\"Scraping courses\")\n",
    "    courses = []\n",
    "    course_department_urls = get_course_department_urls(cache)\n",
    "    for course_department_url in course_department_urls:\n",
    "        department_courses = scrape_course_department(cache, course_department_url)\n",
    "        courses.extend(department_courses)\n",
    "    return courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function logs a little message to the console and then grabs a list of all of the course department urls. It then goes through each url, downloads and parses that page for its list of courses, and adds them to the total list of courses. How does it scrape the list of course_department_urls? Let's look at the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of urls of each course department\n",
    "def get_course_department_urls(cache: HTMLCache) -> list:\n",
    "    course_department_urls = []\n",
    "    all_courses_soup = cache.get_soup(COURSES_URL)\n",
    "    letter_lists = all_courses_soup.find(id=\"atozindex\").find_all(\"ul\")\n",
    "    for letter_list in letter_lists:\n",
    "        department_urls = scrape_urls(letter_list, COURSES_URL)\n",
    "        course_department_urls.extend(department_urls)\n",
    "    return course_department_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function starts by grabbing the soup of the url COURSES_URL, which has the value of \"http://catalogue.uci.edu/allcourses/\". Why don't you go to this page? You'll see that it has a list of departments, but more importantly, each of these departments links to its own department page. Try clicking on a department page. You'll notice that that department page contains all of the courses in that department with most of the data we need for our database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the COURSES_URL page and open up the inspector (Ctrl+Shift+I for Windows or right click and click on Inspect). If you go to the Elements tab (which is usually the default tab), you'll see that you're looking at the HTML code of the page. If you hover over various elements, that element will highlight on the website. This is how BeautifulSoup sees the website, as its HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, right click on one of the departments and click \"Inspect\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inspect1](inspect1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look on the Inspector on the right now, you'll see that it jumped to where the element is in the HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inspect2](inspect2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The element is an anchor (\\<a\\>) element, which means that it stores a link to a different page. You can see which page it stores a link to in its \"href\" tag, which in this case leads to /allcourses/ac_eng/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might say, what kind of path is /allcourses/ac_eng/? Well, it's not a full url, it's supposed to be stuck at the end of the current host, which is https://catalogue.uci.edu/. If you stick the host and the path together, you get: https://catalogue.uci.edu/allcourses/ac_eng/, which is the url of the department page, which we can then download and scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our battle plan is this, look for all of these anchor elements, compute the urls of the departments pages using their href tags by sticking the href path onto the host path, and then downloading the courses on those department pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we get there, how the heck do we grab all of these anchor elements and their hrefs? If you read through BeautifulSoup's documentation, you'll find that if you're given a soup, there are 2 important important functions for finding stuff: find and find_all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"find\" function lets you find the first element with a given set of parameters. If you use \"soup.find(\"div\", id=\"bob\")\", then it will return the first div element with the id \"bob\". The \"find_all\" function will return all elements with the given set of parameters. If you use \"soup.find_all(\"ul\")\", then it will return all \"ul\" elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's obviously a lot more features, but with these alone, we can grab all of the anchor elements we need and their href tags. If you notice the structure of the HTML code, these anchor elements live inside list (\\<li\\>) elements, which live inside unordered list (\\<ul\\>) elements, which live inside a div element with id \"atozindex\". This means that if we search for all of the anchor elements inside of these unordered list elements inside of the \"atozindex\" div, we'll get all the anchor tags we need. Let's look back at the get_course_department_urls function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of urls of each course department\n",
    "def get_course_department_urls(cache: HTMLCache) -> list:\n",
    "    course_department_urls = []\n",
    "    all_courses_soup = cache.get_soup(COURSES_URL)\n",
    "    letter_lists = all_courses_soup.find(id=\"atozindex\").find_all(\"ul\")\n",
    "    for letter_list in letter_lists:\n",
    "        department_urls = scrape_urls(letter_list, COURSES_URL)\n",
    "        course_department_urls.extend(department_urls)\n",
    "    return course_department_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by grabbing the soup for the page, and from that soup, we find an element with id=\"atozindex\" (the div we were talking about earlier). Remember that ids are unique across the entire page, so only this particular div will have this id. We then call .find_all(\"ul\") to grab all unordered list elements inside of that div. Now for each of these unordered list divs, we search for all of the anchor elements we need and stick their href tags onto COURSES_URL. Since this process is kinda tedious, I put it in its own utility function, scrape_urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read through scrape_urls, you'll notice that it doesn't look for all of the list item elements (\\<li\\>) first before looking for the anchor elements. That's because BeautifulSoup looks recursively for elements. That means it will look inside the children of an element, and the children of those children, and the children of those children, and so on. Why didn't we just do that from the start? Look for all anchor tags on the page. The problem is that these course department anchor elements aren't the only anchor elements on the page, which means find_all will return too many elements and it would be annoying to look through them all to see if they're what we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grabbing those urls, the get_course_department_urls function appends them all to one list and returns them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to follow the rest of the functions because I feel like they're mostly self explanatory with the information I've given above. Web scraping really comes down to these 2 things:\n",
    "- Inspect the HTML page manually for the elements you're looking for and their patterns\n",
    "- Use BeautifulSoup to find those elements in the HTML code and grab the resources you're looking for from them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we've scraped all of the course data and we have this massive database of data. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of having all this data is to make it searchable by the user. Let's say that the user wants to find the course \"COMPSCI 121\". What we're providing is a search bar that lets the user type in whatever query they want, as well as a couple of filters that we'll talk about later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the problem...the search query can be whatever the user wants. They might type in \"compsci 121\". They might type in \"information retrieval\". They might even type in \"cOmPsCi 121\", and expect the same results for each of these queries. They might even expect different results. How do we make sure that they get the course they're looking for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called Free Text Search, and it has some other names too but I can't remember them. There's no 1 right way to do it, but here's the general formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess/Tokenize everything\n",
    "- Create indexes\n",
    "- (and in our case, because we have those \"filters\") Filter results\n",
    "- Rank results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is standardize all of the text we have. That means lowercasing everything and tokenizing everything. What is tokenizing? Tokenizing is splitting up text into its words, i.e. \"information retrieval\" becomes \"information\", \"retrieval\". This makes it so that we don't have to look for individual characters in our database, we can look for whole words. Usually when you tokenize you get rid of things like punctuation so that users don't have to type it in, i.e. \"That's his\" becomes \"thats\", \"his\". It seems simple enough, but how would you tokenize hyphens? Would you split the word up into two different words? Should you include certain characters like \"+\" for \"C++\"? We can develop this tokenizer as we learn more about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tokenize the user's search query, we should tokenize our database too, and ideally with the same tokenization algorithm. This way, we can see if the tokens in our search query exist in the tokens in our database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how would we search through these tokens? If we have the tokens \"information\" and \"retrieval\" from our search query, would we go through every single course in our database and check to see which ones have those tokens in their text somewhere? This may be fine if we had few enough courses, but you can imagine that looking through the entire database over and over again for each query would be pretty inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where indexes come into play. Indexes are data structures that make searching faster. We can imagine that after we tokenize our course database, it would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     course_id1: [list of tokens1],\n",
    "#     course_id2: [list of tokens2],\n",
    "#     course_id3: [list of tokens3],\n",
    "#     ...\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which case, if we were looking for all of the courses with the tokens \"information\" and \"retrieval\", we could look through every single course id and check if both \"information\" and \"retrieval\" are in their list of tokens, and if they are, add them to some list. We have a little under 6000 courses in the database, which means that each search would have to look inside our database about 6000 times, not including the search for the tokens inside of each tokens list. What if our database had a different structure...what if we flipped things around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     token1: [list of courses_ids1],\n",
    "#     token2: [list of courses_ids2],\n",
    "#     token3: [list of courses_ids3],\n",
    "#     ...\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if, as above, we had each token as an entry, and each value as the list of courses that have that token? Then, all we would have to do to find all courses with tokens \"information\" and \"retrieval\" would be to find those tokens from the database and match up whichever courses are in both lists. That's 2 database searched instead of 6000, which is pretty good in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wouldn't want to restructure our database because we might want it in that original structure for other purposes, but instead we could create a new data structure with the structure that we were just talking about. That would be our index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we use this index, and grab our list of courses that we wanted. Should we just show them as it is? One thing that you should know about search is that users are lazy. They want the result they were looking for at the very top of the list of results. When you look stuff up on Google, how often do you look past the first 3 or 4 results? That's why we have to rank our results by how relevant they are to the user's query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's actually no one way to do this, but there are some popular algorithms like tf-idf. It really should be unique to every search engine. In our case, since we know users are searching for courses, we probably want to put the course with id \"ICS 33\" at the top and courses with prerequisite \"ICS 33\" underneath instead of vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably also want to add some special cases into our index, like acronyms. You probably don't want to have to write \"compsci 121\" into the search bar, maybe you just want to write \"cs 121\". There's really no one way to do search, so we will adapt our algorithm as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you couldn't tell from the length of this tutorial, there's a lot to web development, just in the web scraping/search engine side alone (the front end  and backend are their own beast), so ask questions! Spend some time on it. It'll take a couple of hours just to get one thing in at the beginning, but you'll get better slowly through experience. If you ever want to get on a call and have me walk you through stuff, I'd be happy to. Good luck!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18db06d66b83781e741671a8bbd09812ebc032e21520231392754694b3895265"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
